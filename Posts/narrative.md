## Post-Election Narrative (12.8.20)

### A Primer

2020 is almost over. The people have voted, the models have been reflected upon, but there’s still time to evaluate some common post-election narratives. This cycle was full of hot takes from both sides preceding and in the immediate aftermath of the election. I plan on tackling one of these narratives: that pollsters and modelers failed to “learn” from 2016 and got the same things wrong.

In the post below, I’ll:

(1) Elaborate on the take,
(2) Craft some testable implications of the narrative,
(3) Gather some data to test that implication,
And (4) Test that narrative and present conclusions.

### The Take

Many observers of the 2020 election felt cheated by the coverage that dominated news cycles beforehand. To these people, Biden seemed like a sure thing, and the Democrats were likely to dominate congressional races as well. Reality disappointed.

Even though most data journalists advised caution (like some had before 2016), there was a collective outpouring of blame toward the analytics industry for failing to anticipate a close election. Some of these broadsides accused quantitatively-focused writers of failing to see what was happening “on the ground.” Others were more forceful, asserting that high poll numbers were tantamount to voter suppression.

Generally, though, it appeared that analytics failed to “learn” from 2016. Four years ago, many were stunned by President Trump’s electoral college victory. Pollsters and modelers claimed to have accounted for some of that error going forward by weighing by education and emphasizing uncertainty in forecasts. However, for many, 2020 was evidence that those fixes didn’t work. For these people, **the polls got the same things wrong again**.

### Testable Implications

How can that narrative be evaluated *empirically*?

### Data Sources

### The Test

### Conclusions
